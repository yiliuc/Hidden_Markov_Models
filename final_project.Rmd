---
title: "Hidden Markov Models for Time Series Analysis"
author: "Yiliu Cao"
date: "2025-03-25"
output:
  pdf_document:
    number_sections: true
#thanks: "The codes can be found in sss" 
fontsize: 11pt
header-includes:
  - \usepackage{tikz}
  - \usetikzlibrary{arrows,shapes,positioning}
  - \tikzset{node font = \large\bfseries\sffamily, 
             every node/.append style = {circle, minimum size = 12mm, thick},
             every edge/.append style = {->, very thick, shorten >= 1pt, shorten <= 1pt}}
bibliography: references.bib
csl: apa.csl
---
```{r, include=FALSE, results='hide', message=FALSE}
library(tidyverse)
# source("codes/overall.R")
source("codes/functions.R")
source("codes/book.R")
source("codes/data.R")

# We use mean and sd notation
```

# Introduction

In probability theory and statistics, a Markov Chain is a stochastic process that models a sequence of random variables in which the distribution of the next state depends only on the current state or a limited number of past observations, and we call this structure a Markov process. This model was first introduced by Andrey Markov in the early 20th century [@markov]. Building on this idea, Leonard E. Baum introduced the Hidden Markov Model (HMM), which extends the Markov chain framework by having two sequences of random variables, one to be hidden and another to be observed, with hidden variables modeling as the Markov process[@baum1]. The distribution of the hidden variable at time $t$ only depends on the hidden variable at time $t-1$, with an observed variable that depends on the hidden variable at the same time $t$. Since the hidden states can not be observed directly, the study of HMMs mainly focuses on two key tasks. Firstly, we want to compute the most likely sequence of hidden states given the observed data. Secondly, we want to estimate the parameters from the HMMs, such as transition distribution. This paper will discuss how to achieve the two goals and illustrate the relationship to the Time Series Analysis.

The Hidden Markov Models was firstly proposed by Baum through a series of papers between the 1960s and 1970s. The initial settings of HMMs follows that, suppose $\{X_t\}$ with stochatic matrix $\{a_{ij}\}$ is a s-state Markov Process and define $\{Y_t\}$ as a probabilistic function of $\{X_t\}$ [@baum1]. The conditional probability of $Y_t$ given all the past values of $\{X_t\}$ and $\{Y_t\}$ up to time $t$ is defined as $b_{jk}$. In their first paper, they proved the consistency of the likelihood of the $\{Y_t\}$ and the smoothness property of the expectation of the log-likelihood of $\{Y_t\}$ wrt to the parameter $\boldsymbol\theta$ from $\{a_{ij}\}$ and $\{b_{ij}\}$ [@baum1]. In their later paper in the 1970s, they further introduced an iterative method for estimating the model parameters and proved that the algorithm converges to a local maximum [@baum1]. This method later became known as the Baum-Welch algorithm, which is the EM Algorithm in HMMs [@zucchini]. The parameters $\{a_{ij}\}$ and $\{b_{ij}\}$ are now commonly referred to as the transition distribution and emission distribution, respectively [@zucchini].

One of the popular applications of HMMs in the Time Series Analysis is GARCH-HMMs or Regime-Switching GARCH [@cai]. It captures the volatility process changes, which may depend on some unobserved regimes, and we assume the changes of hidden states following a Markov process. In this hybrid model, the HMMs divide the time series with different volatility levels and then use the GARCH model to model the time-varying variance within each regime [@zhuang]. For example, in financial time series, this allows the model to shift between high and low volatility periods, such as market booms and crashes. We can, therefore, predict the state the next time and choose the corresponding local GARCH model to predict the volatility. This model is advantageous as it allows us to see how volatility changes between different levels and specific GARCH for each level.

Another significant development in HMMs is Automatic Speech Recognition (ASR). It is based on HMMs and became well-known by IBM and Bell Lab in the 1980s [@ibm]. It has been considered an important tool to enhance the performance of human-human and human-machine communications. A famous one is Apple's Siri, which can be acted as a virtual assistant. The basic algorithm for HMMs in ASR is to set hidden states to be the phonetic characters and use the properties of HMMs to identify the true sentence that the speaker is saying. It can even be used to predict what the speaker is trying to say given what he already said. Moreover, one of the most recent studies of ASR uses Deep learning methods such as Deep Neural Networks to construct multiple hidden layers to recognize speech, which are the Deep Neural Networks Markov Models for ASR [@asr].

This paper is structured as follows. Section 2 will briefly review the Markov Chain, along with some properties. Then, Section 3 will introduce the Hidden Markov Model, where we will discuss its definitions and properties. In addition, Section 4 will talk about the Baum-Welch Algorithm, which is the application of the EM algorithm in HMMs. We will also discuss Viterbi Algorithm as well. Section 5 will discuss HMM-GARCH. After that, we will apply this algorithm to real-world scenarios and compare their performance on Section 6. We will also provide some discussion and conclusion in Section 7.

# Markov Chains

## Basics
Markov Chains models the structure of dependence for a sequence of variables. Let $\{X_t:t\in\mathbb{N}\}$ be a sequence of discrete random variables with values being one *state* from the state space $\mathbf{S}=\{1,\ldots,m\}$ s.t. $|\mathbf{S}|=m$, then such a sequence of variables is called a **Markov Chain (MC)** if, for all $t\in\mathbb{N}$, it satisfies the Markov Property such that
$$
P(X_{t+1}|X_t,\ldots,X_1)=P(X_{t+1}|X_t)
$$
The Markov Property says that the probability of a random variable in a MC only depends on the most recent one. With this property, the Markov Chain can be visualized as
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  \node[draw] (x1) {$X_{t-1}$};
  \node[draw, right = of x1] (x2) {$X_t$} edge[<-] (x1);
  \node[draw, right = of x2] (x3) {$X_{t+1}$} edge[<-] (x2);
  \node[draw, right = of x3] (x4) {$X_{t+2}$} edge[<-] (x3);
  \node[right = of x4, draw=none] (dots) {$...$} edge[<-] (x4);
  \node[left = of x1, draw=none] (dots2) {$...$} edge[->] (x1);
\end{tikzpicture}
\caption{First Order Markov Chain}
\label{fig:markov-chain}
\end{figure}
Since each variable is discrete, the probability of transitioning from state $i$ to state $j$ at two consecutive time points is called the **transition probability**, denoted by 
$$
\gamma_{ij}=P(X_{t+1}=j|X_t=i)
$$
If $\gamma_{ij}$ does not change across time, $p(X_{t+1}=j|X_t=i)=p(X_{t+2}=j|X_{t+1}=i)\:\forall t\in\mathbb{N}$, then such markov chains are said to be **homogeneous (stationary)**. The matrix $\boldsymbol{\Gamma}(1)=\boldsymbol{\Gamma}$ is defined as the transition matrix with $(i,j)^{th}$ element as $\gamma_{ij}$. More generally, if the time points are not consecutive, the transition probability over $t$ time steps is given by $\gamma_{ij}(t)=P(X_{s+t}=j|X_s=i)$ with transition matrix $\boldsymbol{\Gamma}(t)$. It can be shown that 
$$
\boldsymbol{\Gamma}(t+u)=\boldsymbol{\Gamma}(t)\boldsymbol{\Gamma}(u)
$$
which is so-called the **Champman-Kolmogorov equations**. (Appendix).

## Stationary Distribution

The unconditional probabilities $u_j(t)=p(X_t=j)$ is the probability of being in the state $j$ at time $t$. It can be expressed as the row vector $\mathbf{u}(t)$ where
$$
\mathbf{u}(t)=\left(p(X_t=1),\ldots,p(X_t=m)\right)=\left(u_1(t),\ldots,u_m(t)\right)\in\mathbb{R}^m
$$
It can be shown that $\mathbf{u}(t+1)=\mathbf{u}(t)\boldsymbol\Gamma$. If the Markov Chain runs for a long time, then we would expect that, starting from some point, the Markov Chain will reach out to a steady state. This means that the subsequent time steps have the same distribution, and we call the distribution at the steady state as the **stationary distribution** of Markov Chain, denoted by $\boldsymbol\delta$. Mathematically, $\boldsymbol\delta$ satisfies
$$
\boldsymbol\delta=\boldsymbol\delta\boldsymbol{\Gamma}
$$
Moreover, it is easy to see that $\boldsymbol\delta$ is the eigenvector of $\boldsymbol\Gamma$ with eigenvalue 1. This also implies that the stationary distribution $\boldsymbol\delta$ may not be unique. In fact, if a markov chain is irreducible (homogeneous, discrete-time, finite state space), then such markov chains has a unique, strictly positive stationary distribution.

In addition, a Markov Chain is said to be **stationary** if it starts at its stationary distribution and continue to have that distribution to all the subsequent time points, implying $\boldsymbol\delta=\mathbf{u}(1)=\cdots=\mathbf{u}(T)$. This is a stronger condition and usually to achieve in practice as it requires to know the exact transition matrix. Besides, the Markov Chain we observed in practice is something that is already running, and we do not have the ability to force it to have the initial state by $\boldsymbol\delta$.

It is important to distinguish between time-homogeneous and stationary Markov Chain. The time-homogeneous Markov Chain says the conditional distribution of hidden variables does not change over time. If a Markov Chain is time-homogeneous and $\mathbf{u}(1)$ is stationary distribution, then this Markov Chain is indeed stationary.

# Hidden Markov Models

**Hidden Markov Models HMMs** is a particular structure of dependence extending from Markov Chain. Besides the observed variables $\left\{X_t: t \in \mathbb{N}\right\}$, there is an extra sequence of hidden variables $\left\{Z_t: t \in \mathbb{N}\right\}$ modelling by Markov Chain and is linked to the observed variables. More specifically, define $\mathbf{Z}^{(t)}$ and $\mathbf X^{(t)}$ representing the histories for hidden and observed variables up to time $t$, the basic HMM follows
$$
\begin{gathered}
p\left(Z_t \mid \mathbf{Z}^{(t-1)}\right)=p\left(Z_t \mid Z_{t-1}\right), \quad t\in\{2,3\ldots\}\\ 
p\left(X_t \mid \mathbf{X}^{(t-1)}, \mathbf{Z}^{(t)}\right)=\operatorname{Pr}\left(X_t \mid Z_t\right), \quad t \in \mathbb{N}
\end{gathered}
$$
A visualization of Basic Hidden Markov Model is shown on Figure 2. Each hidden variable only depends on the previous hidden variable, and the observed variable only depends on the hidden variable at the same time $t$.
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  \node[draw] (z1) {$Z_{t-1}$};
  \node[draw, right = of z1] (z2) {$Z_t$} edge[<-] (z1);
  \node[draw, right = of z2] (z3) {$Z_{t+1}$} edge[<-] (z2);
  \node[draw, right = of z3] (z4) {$Z_{t+2}$} edge[<-] (z3);
  \node[right = of z4, draw=none] (dots) {$...$} edge[<-] (z4);
  \node[left = of z1, draw=none] (dots2) {$...$} edge[->] (z1);
  \node[draw, below = of z1] (x1) {$X_{t-1}$} edge[<-] (z1);
  \node[draw, below = of z2] (x2) {$X_t$} edge[<-] (z2);
  \node[draw, below = of z3] (x3) {$X_{t+1}$} edge[<-] (z3);
  \node[draw, below = of z4] (x4) {$X_{t+2}$} edge[<-] (z4);
\end{tikzpicture}
\caption{Basic Hidden Markov Model}
\label{fig:hidden-markov-chain}
\end{figure}
All the properties of Markov Chain introduced on the Section 2 are now applied to the hidden variables $\left\{Z_t: t \in \mathbb{N}\right\}$ instead of the observed variables. Moreover, $\left\{X_t: t \in \mathbb{N}\right\}$ is called an $m$-state HMM if $|\mathbf{S}|=m$. In addition, since the observed variables only depends on the corresponding hidden variable, such conditional probability is called the **Emission (state-dependent) distribution** $p_i(x)$ such that
$$
p_i(x)=p(X_t=x|Z_t=i)
$$
Intuitively, the emission distribution is distribution of the observed variable that the hidden variable "emits" to it. Therefore, there are $m$ emission distributions, corresponding to each state of the hidden states, whcih does not depend on $t$.

## Marginal Distribution and Likelihood
To estimate the marginal distribution of the observed variables, the method we take is the marginalization that marginalize the hidden variable. Suppose in bivariate cases we have four variables $X_t,X_{t+k},Z_t,Z_{t+k}$, the joint distribution of $p(X_t=a,X_{t+k}=b)$ can be expressed as
$$
p(X_t=a,X_{t+k}=b)=\sum_{i}^m\sum_{j}^m p(X_t=a,X_{t+k}=b,Z_t=i,Z_{t+k}=j)
$$
By the properties of HMMs, $Z_{t+k}$ only depends on $Z_t$ as we can marginalize the intermediate nodes out, the joint distribution of the four variables can be expressed as $p(Z_t=i)p(X_t=a|Z_t=i)p(Z_{t+k}=j|Z_t=i)p(X_{t+k}=b|Z_{t+k}=j)$. Therefore, the joint distribution of incomplete data can be written as
$$
\begin{aligned}
p(X_t = a, X_{t+k} = b)
&= \sum_{i}^m\sum_{j}^m p(Z_t = i) p(X_t = a \mid Z_t = i) p(Z_{t+k} = j \mid Z_t = i) p(X_{t+k} = b \mid Z_{t+k} = j) \\
&= \sum_{i}^m\sum_{j}^m u_i(t) p_i(a) \gamma_{ij}(k) p_j(b)\\
&=\mathbf{u}(t) \mathbf{P}(a) \boldsymbol{\Gamma}^k \mathbf{P}(b) \mathbf{1}^{\prime}
\end{aligned}
$$
where $\mathbf{P}(x) = \operatorname{diag}(p_1(x), p_2(x), \dots, p_m(x))$. This reduces to $\boldsymbol\delta\mathbf{P}(a) \boldsymbol{\Gamma}^k \mathbf{P}(b) \mathbf{1}^{\prime}$ if the Markov Chain is stationary as we assume the distribution of hidden states at any time $t$ is the stationary distribution $\boldsymbol\delta$

Using the bivariate cases, we can extend it to all observed variables $\{X_t:t\in\{1,\ldots,T\}\}$. Suppose we have initial distribution $\boldsymbol\delta$ and emission probability $p_i(x)$, the likelihood function of observed variables $L_T=p(X_1=x_1,\ldots,X_T=x_T)$ can be written as
$$
L_T=\boldsymbol\delta \mathbf{P}\left(x_1\right) \boldsymbol{\Gamma} \mathbf{P}\left(x_2\right) \boldsymbol{\Gamma} \mathbf{P}\left(x_3\right) \cdots \boldsymbol{\Gamma} \mathbf{P}\left(x_T\right) \mathbf{1}^{\prime}
$$
If the Markov Chain is stationary, we can replace $\boldsymbol\delta$ by $\boldsymbol{\delta}\mathbf\Gamma$. Note that $\boldsymbol\delta$ usually represents the stationary distribution but however always hard to estimate. With that said, $\boldsymbol\delta$ here represents the distribution of the first hidden variable $Z_1$.\

# EM Algorithm in HMMs

The Expectation-Maximization (EM) algorithm is a recursive algorithm to perform the Maximum Likelihood Estimation when some values are missing. It is advantagenous when likelihood function is complicated and direct maximum likelihood estimation is inappropriate. It firstly calculate the expected likelihood of complete data given the incompleted (observed) data and current estimate of $\boldsymbol\theta$ (expectation). It then then maximize the expected likelihood to update $\hat{\boldsymbol\theta}$ (maximization). We run this for multiple time until convergence, and hence find the estimates. The resulting estimates of $\boldsymbol\theta$ is hence a stationary point of the observed data.

EM algorithm can also be applied to HMMs in a very straightforward way. As the hidden states can not be observed directly, which can be naturally treated as "missing." We can then apply the EM algorithm to recursively estimate $\boldsymbol\theta$, which is so-called the **Baum-Welch Algorithm**. This section will discuss both Expectation and Maximization step, respectively.

## Forward-Backward Algorithm

The Forward-Backward Algorithm is one of the fundamental procedure to compute the posterior probability of the hidden states at each time step given the full sequence of observed data. In formula, it evaluates $p\left(Z_t=z_t\mid \mathbf{X}^{T}=\mathbf{x}^{T}\right)$, which is crucial as the hidden state are not directly observable. It relies on the two important quantities: **forward probabilities** and **backward probabilities**, defined as $\alpha_t(j)=p(\mathbf{X}^{(t)}=\mathbf{x}^{(t)},Z_t=j)$ and $\beta_t(i)=p(\mathbf{X}^T_{t+1}=\mathbf{x}^T_{t+1}|Z_t=i)$. The names of "forward" and "backward" comes from the direction in which each quantity transfer the information. The forward probabilities accumulates the information from $t=1$ to the present time $t$, while the backward probabilities integrate the information from the future $t=T$ to the present. In this part, we will discuss how the two quantities are defined in this way, and eventually show that the posterior probability is proportional to their products.

Both $\alpha_t(i)$ and $\beta_t(i)$ is came naturally from the factorization of the likelihood function. In matrix form, the forward probabilities $\alpha_t$ can be expressed as
$$
\boldsymbol{\alpha}_t =\boldsymbol{\delta} \mathbf{P}\left(x_1\right) \prod_{s=2}^t \boldsymbol{\Gamma} \mathbf{P}\left(x_s\right)
$$
which we can show that $\alpha_t(j)=p(\mathbf{X}^{(t)}=\mathbf{x}^{(t)},Z_t=j)$. Similarly, the backward probabilities can be expressed as
$$
\boldsymbol{\beta}_t^{\prime}=\left(\prod_{s=t+1}^T \boldsymbol{\Gamma} \mathbf{P}\left(x_s\right)\right)\mathbf{1}^{\prime}
$$
implying $\beta_t(i)=p(\mathbf{X}^T_{t+1}=\mathbf{x}^T_{t+1}|Z_t=i)$ (Appendix). As we will show later, the posterior probability over the hidden state is proportional to the product $\alpha_t(j)\beta_t(i)$.

The task of hidden state inference from Forward-Backward Algorithm breaks down into three parts: **filtering**, **prediction** and **smoothing**. In filtering, the goal is to compute the posterior probability of the current hidden state at the current time $t$, given all observations up to that point. This is equivalent to *filtered* marginals $\tilde{\alpha}_t(j)=p(Z_t=j|\mathbf{X}^{(t)}=\mathbf{x}^{(t)})\propto\alpha_t(j)$. For example, the posterior over $Z_t$ given the observations up to $t-1$ can be expressed by
$$
\begin{aligned}
P(Z_t = j \mid \mathbf{x}^{(t-1)}) 
&= \sum_i P(Z_{t+1} = i, Z_t = j \mid \mathbf{x}^{(t+1)}) \\
&= \sum_i P(Z_t = j \mid Z_{t+1} = i, \mathbf{x}^{(t+1)}) \, P(Z_{t+1} = i \mid \mathbf{x}^{(t+1)}) \\
&= \sum_i P(Z_t = j \mid Z_{t+1} = i) \, \alpha_{t+1}(i) \\
&= \sum_i \gamma_{ij} \, \alpha_{t+1}(i) \\
&= \left(\alpha_{t+1}^T\Gamma\right)_j
\end{aligned}
$$
This idea can be extended to estimate $P(Z_{t+k} = j \mid \mathbf{x}^{(t)})$ for $k>1$.

Finally, the smoothing provides a refined estimate of the hidden state at time $t$  by conditioning on the entire data $\mathbf{x}^{(T)}$. The smoothed posterior probability is given by
$$
\begin{aligned}
p\left(z_t \mid \mathbf{x}^T\right) &\propto p\left(z_t, \mathbf{x}^T\right) \\
&=p\left(z_t, \mathbf{x}^t, \mathbf{x}^T_{t+1}\right) \\
&=p\left(z_t, \mathbf{x}^t\right) p\left(\mathbf{x}^T_{t+1} \mid z_t, \mathbf{x}^t\right) \\
&=p\left(z_t, \mathbf{x}^t\right) P\left(\mathbf{x}^T_{t+1} \mid z_t\right)\\
&=\alpha_t(z_t)\beta_t(z_t)
\end{aligned}
$$
By combining the forward and backward probabilities, it provides us an effective way to estimate the hidden state for any time $t$ given the entire observed data. It can be used to find the "beliefs" of the hidden state. We will show later that the Forward-Backward Algorithm is closely related to the E-step in the EM Algorithm and it can be also used as the local decoding on the next session.

## Expectation Step

In the Expectation-Maximization (EM) algorithm for Hidden Markov Models, the E-step involves computing the expectation of the complete-data log-likelihood given the observed data and the current parameter estimates. In the context of HMMs, the complete data combines both the observed data $x_t$ and the corresponding hidden states $z_t$. Since the hidden states are not directly observed, it naturally becomes the missing part. We will firstly express the complete-data likelihood, and then evaluate its expectation conditional on the observed data sequence.

Let the parameters be $\boldsymbol\theta = \left( \boldsymbol\delta, \boldsymbol\Gamma, \boldsymbol\lambda\right)$, where $\boldsymbol\delta$ is the initial distribution, $\boldsymbol\Gamma$ is the transition matrix with $(i,j)th$ element $\gamma_{ij}$, and $\boldsymbol\lambda$ be the emission distribution specific parameters. Let the complete data be $\mathcal{D}_{complete}=\left\{ \mathbf{X}^{(T)} = \mathbf{x}^{(T)}, \mathbf{Z}^{(T)} = \mathbf{z}^{(T)} \right\}$. Using the standard factorization of joint distribution of HMMs, the log-likelihood function can be written as
$$
\begin{aligned} 
\ell\left(\boldsymbol\theta|\mathcal{D}_{complete}\right)&=\log\left(p\left(z_1\right) \prod_{t=2}^T p\left(z_t | z_{t-1}\right) \prod_{t=1}^T p\left(x_t | z_t\right)\right)\\
& =\log \delta_{z_1}+\sum_{t=2}^T \log \gamma_{z_{t-1}, z_t}+\sum_{t=1}^T \log p_{z_t}\left(x_t\right)
\end{aligned}
$$
However, the log-likelihood expression above is written in terms of specific hidden states $z_t$, and does not explicitly leading to the parameters $\boldsymbol\theta$. To solve this, we introduce two new indicator variables $u_j(t) = \mathbf{1}\{Z_t = j\},\:t\in\{1,\ldots,T\}$ and $v_{jk}(t) = \mathbf{1}\{Z_{t-1} = j, Z_t = k\},\:t\in\{2,\ldots,T\}$. By plugging in the indicators, the log-likelihood function becomes
$$
\ell\left(\boldsymbol\theta|\mathcal{D}_{complete}\right) = \sum_{j=1}^m u_j(1) \log \delta_j+\sum_{j=1}^m \sum_{k=1}^m\left(\sum_{t=2}^T v_{j k}(t)\right) \log \gamma_{j k}  +\sum_{j=1}^m \sum_{t=1}^T u_j(t) \log p_j\left(x_t\right)
$$
The two indicator variables depends only on the hidden states which is unobserved. On the other hand, the rest term on the log-likelihood are all parameters which is not random. Therefore, in the E-step, we only need to compute the expectation of $u_j(1),v_{jk}(t),u_j(t)$ conditioning on the observed data $\mathbf{x}^{(T)}$ and the current estimates of $\boldsymbol\theta$. It can be shown that these expectations are (Appendix)
$$
\begin{aligned}
\mathbb{E}[u_j(t)|\mathbf{x}^{(T)},\hat{\boldsymbol\theta}]&=p\left(C_t=j\mid\mathbf{x}^{(T)},\hat{\boldsymbol\theta}\right)\\
&=\hat\alpha_t(j)\hat\beta_t(j)/\hat{L}_T\\
&=:\hat{u}_j(t)
\end{aligned}
$$
$$
\begin{aligned}
\mathbb{E}[v_{j k}(t)|\mathbf{x}^{(T)},\hat{\boldsymbol\theta}]&=p\left(C_{t-1}=j, C_t=k \mid \mathbf{x}^{(T)},\hat{\boldsymbol\theta}\right)\\
&=\hat\alpha_{t-1}(j)\hat\gamma_{j k}\hat{p}_k\left(x_t\right)\hat\beta_t(k) / \hat{L}_T\\
&=:\hat{v}_{j k}(t)
\end{aligned}
$$
where the estimated forward and backward probabilities are estimated by
$$
\begin{gathered}
\hat\alpha_{t+1}(j)=\sum_{i=1}^m \hat\alpha_t(i)\hat\gamma_{ij}\hat{p}_j\left(x_{t+1}\right)\text{ s.t. }\alpha_1(j)=\delta_j p_j\left(x_1\right)\\ \hat\beta_t(i)=\sum_{j=1}^m\hat\gamma_{ij}\hat{p}_j\left(x_{t+1}\right)\hat{\beta}_{t+1}(j)\text{ s.t. }\beta_T(j)=1
\end{gathered}
$$
And the estimated likelihood is given by
$$
\hat{L}_T=\hat{\boldsymbol\alpha}_T\hat{\boldsymbol\beta}_T^\prime
$$
Putting all together, the expected log-likelihood function is written as
$$
\mathbf{Q}(\boldsymbol\theta,\hat{\boldsymbol\theta})=\sum_{j=1}^m\hat{u}_j(1)\log \delta_j+\sum_{j=1}^m \sum_{k=1}^m\left(\sum_{t=2}^T \hat{v}_{j k}(t)\right) \log \gamma_{j k}  +\sum_{j=1}^m \sum_{t=1}^T\hat{u}_j(t)\log p_j\left(x_t\right)
$$

## Maximization Step

In M step, we maximize the expected log-likelihood $\mathbf{Q}(\boldsymbol\theta,\hat{\boldsymbol\theta})$ wrt to $\boldsymbol\theta$ to update the estimates
$$
\hat{\boldsymbol\theta}^{new}=\arg \max _\theta \mathbf{Q}(\boldsymbol\theta,\hat{\boldsymbol\theta})
$$
Conveniently, the subjective function separates to three terms with each depending on a component of $\boldsymbol\theta$: the initial distributions, transition matrix and emission parameters. This allows us to maximize each set of independent parameters. Taking the derivatives for each time, we can obtain the closed-form updates for $\boldsymbol\delta$ and $\boldsymbol\Gamma$ as
$$
\begin{gathered}
\delta_j=\frac{\widehat{u}_j(1)}{\sum_{j=1}^m \widehat{u}_j(1)}=\widehat{u}_j(1)\\
\gamma_{j k}=f_{j k} / \sum_{k=1}^m f_{j k},\text{ where }f_{j k}=\sum_{t=2}^T \widehat{v}_{j k}(t)
\end{gathered}
$$
However, the estimates for the emission parameters $\boldsymbol\lambda$ depends on the specific form of the emission model and we can not provide a general formula here.

We will run EM Algorithm recursively until convergence.

# Forecast and Decoding

Forecast and decoding are two important implications from HMMs, which can help us to predict the 

## Forecasting

The forecasting in HMMs means we want to predict the values of the observed variables given the information we have. It describes the conditional distribution of the $X_{T+h}$ given all the observed values we have up to time $T$. The conditional distribution can be expressed as
$$
\begin{aligned}
p(X_{T+h}=x|\mathbf{X}^{(T)}=\mathbf{x}^{(T)})&=\frac{p(X_{T+h}=x,\mathbf{X}^{(T)}=\mathbf{x}^{(T)})}{p(\mathbf{X}^{(T)}=\mathbf{x}^{(T)})}\\
&=\frac{\boldsymbol\delta \mathbf{P}\left(x_1\right)\cdots \boldsymbol{\Gamma} \mathbf{P}\left(x_T\right)\boldsymbol{\Gamma}^h \mathbf{P}\left(x\right)\mathbf{1}^{\prime}}{\boldsymbol\delta \mathbf{P}\left(x_1\right)\cdots \boldsymbol{\Gamma} \mathbf{P}\left(x_T\right)}\\
&=\frac{\boldsymbol\alpha_T\boldsymbol{\Gamma}^h \mathbf{P}\left(x\right)\mathbf{1}^{\prime}}{\boldsymbol\alpha_T\mathbf{1}^{\prime}}
\end{aligned}
$$
Let $\boldsymbol\phi_T=\frac{\boldsymbol\alpha_T}{\boldsymbol\alpha_T\mathbf{1}^{\prime}}$, then it can be written as $\phi_T\boldsymbol\Gamma^h\mathbf{P}(x)\mathbf{1}^{\prime}$. It can be also written in terms of a mixture of the emission distribution $p\left(X_{T+h}=x \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)=\sum_{i=1}^m \xi_i(h) p_i(x)$, where $\xi_i(h)$ is $i$th entry of the vector $\phi_T\boldsymbol\Gamma^h$.

One important implication of the forecasted conditional distribution is that, as $h$ increases, the forecast distribution converges to the marginal distribution of the stationary HMM, that is
$$
\lim _{h \rightarrow \infty} \operatorname{Pr}\left(X_{T+h}=x \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)=\lim _{h \rightarrow \infty} \boldsymbol{\phi}_T \boldsymbol{\Gamma}^h \mathbf{P}(x) \mathbf{1}^{\prime}=\boldsymbol{\delta}^* \mathbf{P}(x) \mathbf{1}^{\prime}
$$
where $\boldsymbol{\delta}^*$ is the stationary distribution and $\boldsymbol{\delta}$ is the initial distribution. It shows that for any non-negative row vectors with row sum 1 ($\boldsymbol{\delta}^*$ here) approaches the stationary distribution of the Markov Chain. However, this property can be only applied to irreducible and aperiodic Markov Chain []. We call $\boldsymbol{\delta}^* \mathbf{P}(x) \mathbf{1}^{\prime}$ as the limiting distribution of the Markov Chain. The speed of converging to the limiting distribution may various. We will discuss more about this in the application part.

## Decoding

In HMMs, the term "forecasting" usually infers to predict the future values of the observed variables $X_t$. In contrast, "decoding" describes the way we use the observed values to compute the most likely hidden states. In this paper, we are going to discuss two kinds of decoding process: local and global decoding. The difference between these two is the number of hidden state we compute each time. If we only decode on hidden state, then it refers the local decoding. If we decode several hidden states simultaneously, it is refered as global decoding.

### Local Decoding

The algorithm for local decoding is simple. Recall that the joint distribution $p\left(Z_t=i, \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)$ can be expressed as the product of forward and backward probabilities, $\alpha_t(i) \beta_t(i)$. Then the conditional distribution of being on state $i$ at time $t$ is
$$
p\left(Z_t=i \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right) =\frac{p\left(Z_t=i, \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)}{p\left(\mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)}
$$
We have proved that $p\left(Z_t=i, \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)=\alpha_t(i) \beta_t(i)$ from the Forward-Backward Algorithm, then the joint distribution can be written as
$$
p\left(Z_t=i \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right) =\frac{\alpha_t(i) \beta_t(i)}{L_T}
$$

Then for each $t\in\{1,\ldots,T\}$, we can compute the most probable hidden state $Z_t=i^*$ by 
$$
i_t^*=\underset{i=1, \ldots, m}{\operatorname{argmax}} p\left(Z_t=i \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)
$$
By this approach, we can find the most likely hidden state at any time $t\leq T$, and this approach is called the local decoding.

### Global Decoding

In contrast to local decoding, global decoding allows us to compute a sequence of hidden states together instead of a single hidden state. It is noticeble that global decoding is not equivalent to run the local decoding for multiple times. The difference between the two is that running local decoding multiple times may give us unrealistic sequence such as impossible transition. We prefer global decoding as it will give us a probable and reasonable sequence of hidden states, while still keep the realistic transition probabilities.

In global decoding, our goal is to to maximize
$$
\begin{aligned}
p(\mathbf{Z}^{(t)}=\mathbf{z}^{(t)}|\mathbf{X}^{(t)}=\mathbf{x}^{(t)})&\propto p(\mathbf{Z}^{(t)}=\mathbf{z}^{(t)}|\mathbf{X}^{(t)}=\mathbf{x}^{(t)})\\
&=\delta_{z_1} \prod_{t=2}^T \gamma_{z_{t-1}, z_t} \prod_{t=1}^T p_{z_t}\left(x_t\right)
\end{aligned}
$$
The complete global decoding works as follows

1. We firstly define $\xi_{1,i}=p(Z_1=i,X_1=x1)=\delta_i p_i(x_1)$, meaning that we are at state $i$ at time $1$.
2. For $t\in\{2,3,\ldots,T\}$, we define
$$
\xi_{t i}=\max _{z_1, z_2, \ldots, z_{t-1}} p\left(\mathbf{Z}^{(t-1)}=\mathbf{z}^{(t-1)}, Z_t=i, \mathbf{X}^{(t)}=\mathbf{x}^{(t)}\right)
$$
$\xi_{t i}$ is the probability of the best sequence of $Z_1,\ldots,Z_t$ such that this sequence has to be ended on $Z_t=i$. It is computed by considering each possible path of $z_1,\ldots,z_t=i$ and return the highest probability. This equation is equivalent to
$$
\xi_{t j}=\left(\max _i\left(\xi_{t-1, i} \gamma_{i j}\right)\right) p_j\left(x_t\right)
$$
By this step, it can give us a $T\times m$ matrix with each element storing the best score at in each time and state.
3. Given $t=T$, we have
$$
i_T=\underset{i=1, \ldots, m}{\operatorname{argmax}}\:\xi_{T i}
$$
This step helps us to find the most likely state at the final time $T$. It can be done simply by search the last row of the matrix we had and state corresponding to the highest score will be the choice of the value of the last hidden state.
4. For $t\in\{T-1,T-2,\ldots,1\}$, find $i_t$ such that
$$
i_t=\underset{i=1, \ldots, m}{\operatorname{argmax}}\left(\xi_{t i} \gamma_{i, i_{t+1}}\right)
$$
Then can have the sequence of values of hidden states.

This algorithm is so-called the Viterbi Algorithm.

### State Prediction

Finally, we are also concerned about how to predict the value of hidden states in the future time $t>T$, $p\left(Z_t=i \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)$. Since $\boldsymbol\alpha_T=p(Z_T,\mathbf{X}^{(T)}=\mathbf{x}^{(T)})$, then the joint distribution can be expressed as $p\left(Z_{T+1},\mathbf{X}^{(T)}\right)=p\left(C_{T+1}=k \mid C_T=j\right)p\left(C_T=j \mid \mathbf{X}^{(T)}\right)=\boldsymbol\alpha_T\boldsymbol\Gamma$. Similarly, we also have $p\left(Z_{T+h},\mathbf{X}^{(T)}\right)=\boldsymbol\alpha_T\boldsymbol\Gamma^h$. To have $p\left(Z_{T+h}=i,\mathbf{X}^{(T)}\right)$, it is equivalent to the $i$-th component of $\boldsymbol\alpha_T\boldsymbol\Gamma^h$, which is equivalent to $\boldsymbol{\alpha}_T \boldsymbol{\Gamma}^h \mathbf{e}_i^{\prime}$, where $\mathbf{e}_i$ is a row vector that only the ith element is 1 and all other are zeros. Therefore the conditional distribution can be written as
$$
p\left(Z_t=i \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)=\frac{p\left(Z_t=i,\mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right)}{\boldsymbol\alpha_T\mathbf{1}^\prime}=\frac{\boldsymbol{\alpha}_T \boldsymbol{\Gamma}^h \mathbf{e}_i^{\prime}}{\boldsymbol\alpha_T\mathbf{1}^\prime}=\phi_T \boldsymbol{\Gamma}^h \mathbf{e}_i^{\prime}
$$
To sum up, recall that in the Forward-Backward Algorithm, we have derived that $p(Z_t=z_t|\mathbf{X}^{(T)}=\mathbf{x}^{(T)})=\alpha_t(z_t)\beta_t(z_t)$, the forecasting of hidden states can be summarized by
$$
\begin{aligned}
L_T \operatorname{Pr}\left(C_t\right. & \left.=i \mid \mathbf{X}^{(T)}=\mathbf{x}^{(T)}\right) \\
& = \begin{cases}\boldsymbol{\alpha}_T \boldsymbol{\Gamma}^{t-T} \mathbf{e}_i^{\prime} & \text { for } t>T \\
\alpha_T(i) & \text { for } t=T \\
\alpha_t(i) \beta_t(i) & \text { for } 1 \leq t<T\end{cases}
\end{aligned}
$$

# Application

It is easy to performing HMMs in R. Although we can still write the recursive function manually, we can still the R package `HiddenMarkov` [] which performs the EM in Hidden Markov Models. In this section, we will firstly simulate the data for both discrete and continuous cases, to see how the EM Algorithm and decoding are performed. We will then apply it to a real-world scenarios such as the volatility of NVIDIA stock.

## Toronto Bicycle Thefts

```{r, echo=FALSE}
n <- nrow(bicycle_data)
x <- bicycle_data$count
plot(c(1:n), bicycle_data$count, type = "l")
```


```{r, include=FALSE}
# Use EM
library(HiddenMarkov)

################################################################################
# # Number of states
# nstates <- 2
# 
# # Transition matrix (same as before)
# Pi <- matrix(c(0.7, 0.3,
#                0.4, 0.6), nrow = nstates, byrow = TRUE)
# 
# # Initial state distribution
# delta <- rep(1 / nstates, nstates)
# 
# # Poisson emission parameters (lambda values for each state)
# lambda <- c(110, 350)

################################################################################
# Number of states
nstates <- 3

# Transition matrix (same as before)
Pi <- matrix(c(0.6, 0.3, 0.2,
               0.5, 0.2, 0.4,
               0.4, 0.3, 0.2), nrow = nstates, byrow = TRUE)

# Initial state distribution
delta <- rep(1 / nstates, nstates)

# Poisson emission parameters (lambda values for each state)
lambda <- c(100, 300, 500)


# Create the HMM with Poisson emissions
model <- dthmm(bicycle_data$count, Pi = Pi, delta = delta, distn = "pois",
               pm = list(lambda = lambda))

fitted_model_bicycle <- BaumWelch(model)
summary(fitted_model_bicycle)
```

```{r, echo = FALSE, fig.width = 8, fig.height = 5, fig.cap = "Local decoding (top) and glocal decoding (bottom) for the number of monthly bicycle thefts at Toronto since 2014 (3 hidden states)"}
mod_new <- convert.HiddenMarkov.output(fitted_model_bicycle, x = x)
state_probs <- pois.HMM.state_probs(x, mod_new)

par(mfrow = c(2, 1), mar = c(1.9, 4, 1.9, 2))
################################################################################
# 1. Local decoding
local_decoding <- pois.HMM.local_decoding(x, mod_new)

# Map decoded states to estimated Poisson means (lambda)
decoded_means <- mod_new$lambda[local_decoding]

# Plot observed data (Poisson counts)
plot(c(1:n), bicycle_data$count,
     type = "l", col = "grey40", lwd = 1,
     ylab = "Bicycle Thefts", xlab = "", main = "Local Decoding")

# Overlay the expected lambda values per decoded state
points(c(1:n), decoded_means, pch = 19, cex = 0.4)

# Optionally, add horizontal reference lines
abline(h = mod_new$lambda, col = "grey", lty = 2)
################################################################################
# 2. Global decoding
global_decoding <- pois.HMM.viterbi(x, mod_new)
# Map decoded states to estimated Poisson means (lambda)
decoded_means <- mod_new$lambda[global_decoding]

# Plot observed data (Poisson counts)
plot(c(1:n), bicycle_data$count,
     type = "l", col = "grey40", lwd = 1,
     ylab = "Bicycle Thefts", xlab = "Time", main = "Global Decoding")

# Overlay the expected lambda values per decoded state
points(c(1:n), decoded_means, pch = 19, cex = 0.4)

# Optionally, add horizontal reference lines
abline(h = mod_new$lambda, col = "grey", lty = 2)
```

```{r, echo=FALSE, fig.cap = "Forecasted distribution of monthly bicycle theft counts in Toronto for the next nine months"}
xf <- 0:500  # finer grid

m <- 3
lambda <- mod_new$lambda
delta <- solve(t(diag(m) - mod_new$gamma + 1), rep(1, m))
dstat <- numeric(length(xf))
for (j in 1:m) {
  dstat <- dstat + delta[j] * dpois(xf, lambda[j])
}

h <- 9
forecasts <- pois.HMM.forecast(xf, h, x, mod_new)

par(mfrow = c(3, 3), las = 1, mar = c(4, 4, 2, 1))  # tidy layout

for (i in 1:h) {
  fc <- forecasts[i, ]
  
  plot(xf, fc,
       type = "h",
       main = bquote("Forecast density for " ~ X[T+.(i)]),
       xlim = c(0, 500),
       ylim = c(0, max(fc) * 1.05),  # auto-scaled per panel
       xlab = "t",
       ylab = "",
       lwd = 1,
       col = "lightblue")
  lines(xf, dstat, col = "black", lwd = 1)
  
  # Optional: vertical line for reference (e.g., last observed value)
  # abline(v = tail(x, 1), col = "red", lty = 2)
}
```

```{r, echo=FALSE}
states_prediction <- pois.HMM.state_prediction(h = 10, x, mod_new)
states_prediction
```

## NVIDIA stock price

The previous simulation example illustrate that the perform well. To further illustrate its performance, let us apply it to the NVIDIA stock price. The data used here is through the R package `quantmod` to access the NVIDIA stock price from January 2nd, 2024 to April 8th, 2025. The goal of this study is to use HMM to estimate the volatility level behind each price and assumed hidden. The volatility level here is calculated by absolute log returns. We divide the volatility levels into three states: low, medium and high, and the emission distribution of volatility is assumed to follow a normal distribution.

```{r, echo = FALSE, warning = FALSE, fig.cap = "The absolute log return (top), ACF plot (bottom left) and PACF (bottom right)"}
x <- na.omit(abs(diff(log(nvda_vec))))
# x <- na.omit(diff(nvda_vec))
n <- length(x)

library(ggplot2)
library(patchwork)
library(forecast)

x <- na.omit(abs(diff(log(nvda_vec))))
ts_plot <- autoplot(ts(x)) +
  labs(x = "Time", y = "Abs. Log Return") +
  ggtitle("")

# ACF plot with custom labels and no title
acf_plot <- ggAcf(x) +
  labs(x = "Lag", y = "ACF") +
  ggtitle("")

# PACF plot with custom labels and no title
pacf_plot <- ggPacf(x) +
  labs(x = "Lag", y = "PACF") +
  ggtitle("")

(ts_plot / (acf_plot | pacf_plot))  # patchwork layout
```

Descriptions here.
```{r, include = FALSE}

library(HiddenMarkov)

################################################################################
# Number of states
nstates <- 3

# Initialize the transition probabilities
Pi <- matrix(c(0.8, 0.1, 0.1,
               0.1, 0.8, 0.1,
               0.1, 0.1, 0.8), nrow=nstates, byrow=TRUE)

# Initialize the initial distribution
delta <- rep(1/nstates, nstates)

# Initial emission parameters
mu <- c(0.01, 0.05, 0.09)
sigma <- c(0.02, 0.02, 0.02)
################################################################################
# # Number of states
# nstates <- 2
# 
# # Initialize the transition probabilities
# Pi <- matrix(c(0.6, 0.4,
#                0.4, 0.6), nrow=nstates, byrow=TRUE)
# 
# # Initialize the initial distribution
# delta <- rep(1/nstates, nstates)
# 
# # Initial emission parameters
# mu <- c(0.02, 0.06)
# sigma <- c(0.02, 0.02)
################################################################################
model <- dthmm(x, Pi=Pi, delta=delta, distn="norm",
               pm=list(mean=mu, sd=sigma))
fitted_model_gaussian <- BaumWelch(model)
summary(fitted_model_gaussian)
```

By performing the EM Algorithm, the estimated initial distribution $\boldsymbol\delta=(1, 0, 0)$, and the estimated transition matrix is
$$
\hat{\boldsymbol\Gamma} =
\begin{pmatrix}
0.254 & 0.678 & 0.068 \\
0.264 & 0.558 & 0.178 \\
0.157 & 0.613 & 0.23
\end{pmatrix}
$$

The estimated parameters are $\mu_1=0.006,\sigma_1=0.004$, $\mu_2=0.025,\sigma_2=0.012$ and $\mu_3=0.062,\sigma_3=0.033$.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Local decoding (top) and glocal decoding (bottom) for the volatility of NVIDIA stock (the most recent 150 observations due to visualbility) (3 hidden states)", fig.width = 8, fig.height = 5}

mod_new <- convert.HiddenMarkov.output(fitted_model_gaussian, x = x)
state_probs <- normal.HMM.state_probs(x, mod_new)

par(mfrow = c(2, 1), mar = c(1.9, 4, 1.9, 2))
################################################################################
# 1. Local Decoding
local_decoding <- normal.HMM.local_decoding(x, mod_new)

# Map decoded states to estimated Poisson means (lambda)
decoded_means <- mod_new$mean[local_decoding]

# Plot observed data (Poisson counts)
plot(1:150, tail(x, 150),
     type = "l", col = "grey40", lwd = 0.6,
     ylab = "", xlab = "", main = "Local Decoding")

# Overlay the expected lambda values per decoded state
points(1:150, tail(decoded_means, 150), pch = 19, cex = 0.2)

# Optionally, add horizontal reference lines
abline(h = mod_new$mean, col = "black", lty = 2, lwd = 0.5)
################################################################################
# 2. Global Decoding
global_decoding <- normal.HMM.viterbi(x, mod_new)

# Map decoded states to estimated Poisson means (lambda)
decoded_means <- mod_new$mean[global_decoding]

# Plot observed data (Poisson counts)
plot(1:150, tail(x, 150),
     type = "l", col = "grey40", lwd = 0.6,
     ylab = "", xlab = "Time", main = "Global Decoding")

# Overlay the expected lambda values per decoded state
points(1:150, tail(decoded_means, 150), pch = 19, cex = 0.2)

# Optionally, add horizontal reference lines
abline(h = mod_new$mean, col = "black", lty = 2, lwd = 0.5)
```

Figure 5 shows the estimated hidden states for each time point on NVIDIA stock prices. We can see that the estimates are reasonable, which almost correctly identify the points with high, medium and low volatilities.

```{r, echo=FALSE, fig.cap = "Forecasted distribution of daily volatility (by absolute log returns) of NVIDIA stock for the next nine days"}
xf <- seq(0, 0.2, length.out = 1000)   # finer grid

m <- 3
mean <- mod_new$mean
sd <- mod_new$sd
delta <- solve(t(diag(m) - mod_new$gamma + 1), rep(1, m))

dstat <- numeric(length(xf))
for (j in 1:m) {
  dstat <- dstat + delta[j] * dnorm(xf, mean = mean[j], sd = sd[j])
}

h <- 9
forecasts <- normal.HMM.forecast(xf, h, x, mod_new)

par(mfrow = c(3, 3), las = 1, mar = c(4, 4, 2, 1))  # tidy layout

for (i in 1:h) {
  fc <- forecasts[i, ]
  
  plot(xf, fc,
       type = "n",
       main = bquote("Forecast density for " ~ X[T+.(i)]),
       xlim = c(0, 0.2),
       ylim = c(0, max(fc) * 1.05),  # auto-scaled per panel
       xlab = expression(x),
       ylab = "Density",
       lwd = 1,
       col = "lightblue")
  polygon(c(xf, rev(xf)),               # x-coordinates (forward + reverse)
          c(fc, rep(0, length(fc))),    # y-coordinates (top + 0 base)
          col = "lightblue", border = NA)
  lines(xf, dstat, col = "black", lwd = 1)
}
```

```{r, echo = FALSE}
# Confidence interval
h <- 4
xf <- seq(0, 0.2, length.out = 1000)  # finer grid
forecasts <- normal.HMM.forecast(xf, h, x, mod_new)
# Setup
dx <- diff(xf)[1]  # grid spacing
H <- h             # total forecast steps
results <- data.frame(
  step = 1:H,
  mean = NA,
  lower_95 = NA,
  upper_95 = NA
)

# Loop over all forecast steps
for (i in 1:H) {
  fc <- forecasts[i, ]
  fc <- fc / sum(fc * dx)  # normalize
  
  # Mean
  mu <- sum(xf * fc * dx)
  
  # CDF and quantile function
  cdf <- cumsum(fc * dx)
  qfun <- approxfun(cdf, xf)
  
  # 95% confidence interval
  lwr <- qfun(0.025)
  upr <- qfun(0.975)
  
  # Store results
  results$mean[i] <- mu
  results$lower_95[i] <- lwr
  results$upper_95[i] <- upr
}

# Display the result
print(results)
```

```{r, echo = FALSE}
states_prediction <- normal.HMM.state_prediction(h = 10, x, mod_new)
states_prediction
```


## HMM_GARCH

```{r, echo=FALSE}
# library(MSGARCH)
# library(MSGARCH)
# 
# # Specify two regimes (each regime gets its own spec)
# spec <- CreateSpec(
#   variance.spec = list(model = c("sGARCH", "sGARCH")),
#   distribution.spec = list(distribution = c("norm", "norm")),
#   switch.spec = list(do.mix = FALSE)  # FALSE means HMM-style regime switching
# )
# 
# # Example data (replace with your own)
# x <- na.omit(abs(diff(log(nvda_vec))))  # or just diff(log(nvda_vec)) for returns
# 
# # Fit the model
# fit <- FitML(spec, data = x)
# 
# # View the result
# summary(fit)
# plot(fit)
# 

```

# Discussion

I will finish this part in the final version.


\newpage


# Appendix
I will show all the necessary proofs and coding on the final version.

\newpage


# References









